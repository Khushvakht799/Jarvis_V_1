import numpy as np
import pickle
import hashlib
from sklearn.metrics.pairwise import cosine_similarity

class VerbEmbeddings:
    def __init__(self, embeddings_dir):
        import os
        os.makedirs(embeddings_dir, exist_ok=True)
        self.embeddings, self.vocab, self.reverse_vocab = self.create_embeddings()
    
    def create_embeddings(self):
        print("ğŸ”„ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ embedding-Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ñ Ğ³Ğ»Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²...")
        
        # ĞŸĞ ĞĞ’Ğ˜Ğ›Ğ¬ĞĞ«Ğ™ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¾Ğ¹
        verbs = {
            'ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ': ['create', 'make', 'build', 'generate'],
            'Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ': ['execute', 'run', 'perform', 'do'],
            'Ğ¿Ğ¾ÑÑ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ': ['calculate', 'compute', 'count', 'sum'],
            'Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ': ['import', 'load', 'include', 'require'],
            'Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ': ['show', 'display', 'print', 'output'],
            'ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ': ['save', 'store', 'write', 'export'],
            'ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ': ['delete', 'remove', 'erase', 'clear'],
            'Ğ½Ğ°Ğ¹Ñ‚Ğ¸': ['find', 'search', 'locate', 'discover'],
            'Ğ¾Ñ‚ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ': ['sort', 'order', 'filter', 'arrange'],
            'Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ': ['merge', 'combine', 'join', 'concatenate'],
            'Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ': ['check', 'verify', 'test', 'validate'],
            'Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ': ['generate', 'produce', 'create', 'make'],
            'Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚': ['hello', 'hi', 'greet', 'welcome'],
            'Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ': ['start', 'launch', 'begin', 'initiate'],
            'Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ': ['stop', 'halt', 'end', 'finish']
        }
        
        vocab = {}
        reverse_vocab = {}
        emb_list = []
        
        for idx, (verb, synonyms) in enumerate(verbs.items()):
            vocab[verb] = idx
            reverse_vocab[idx] = verb
            
            # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³
            h = hashlib.md5(verb.encode()).digest()
            emb = np.frombuffer(h[:16], dtype=np.float32)
            emb = np.pad(emb, (0, 384 - len(emb)), 'constant')
            emb = emb / (np.linalg.norm(emb) + 1e-10)
            emb_list.append(emb)
        
        embeddings = np.array(emb_list)
        
        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼
        np.save("data/embeddings/verb_embeddings.npy", embeddings)
        with open("data/embeddings/verb_vocab.pkl", 'wb') as f:
            pickle.dump({'vocab': vocab, 'reverse_vocab': reverse_vocab}, f)
        
        print(f"âœ… Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¾ {len(vocab)} ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²")
        return embeddings, vocab, reverse_vocab
    
    def get_embedding(self, text):
        words = text.lower().split()
        verb_embs = []
        
        for word in words:
            if word in self.vocab:
                idx = self.vocab[word]
                verb_embs.append(self.embeddings[idx])
        
        if verb_embs:
            emb = np.mean(verb_embs, axis=0)
        else:
            emb = np.random.randn(384).astype(np.float32)
            emb = emb / (np.linalg.norm(emb) + 1e-10)
        
        return emb / (np.linalg.norm(emb) + 1e-10)
    
    def find_similar(self, query, top_k=5):
        q_emb = self.get_embedding(query)
        sims = cosine_similarity(q_emb.reshape(1, -1), self.embeddings)[0]
        
        indices = np.argsort(sims)[::-1][:top_k]
        results = []
        for idx in indices:
            if sims[idx] > 0.3:
                verb = self.reverse_vocab[idx]
                results.append((verb, float(sims[idx])))
        
        return results
    
    def get_vocab_size(self):
        return len(self.vocab)